{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainerrl\n",
    "from chainerrl import explorers\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "from time import sleep\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "# COLORS\n",
    "white = (255, 255, 255)\n",
    "black = (0, 0, 0)\n",
    "red = (255, 0, 0)\n",
    "green = (0, 255, 0)\n",
    "blue = (0, 0, 255)\n",
    "purple = (255, 0, 255)\n",
    "\n",
    "# SIZES\n",
    "AGENT_SIZE = 50\n",
    "WEAPON_SIZE = 20\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, name, xy, angle, speed, game_dims=(1000,800)):\n",
    "        self.name = name\n",
    "        self.x, self.y = xy\n",
    "        self.speed = speed\n",
    "        self.angle = math.radians(-angle)  # -1 to 1 \n",
    "        self.dx = self.speed * math.cos(self.angle)\n",
    "        self.dy = self.speed * math.sin(self.angle)\n",
    "        self.dimx, self.dimy = game_dims\n",
    "        \n",
    "    def update(self, agent_xy):\n",
    "        self.x += self.dx\n",
    "        self.y += self.dy\n",
    "\n",
    "        agent_x, agent_y = agent_xy\n",
    "\n",
    "        has_hit_x = self.x >= agent_x - WEAPON_SIZE and self.x <= agent_x + AGENT_SIZE\n",
    "        has_hit_y = self.y >= agent_y - WEAPON_SIZE and self.y <= agent_y + AGENT_SIZE\n",
    "        \n",
    "        exit_boundary = self.x > self.dimx-50 or self.x < 000 or self.y > self.dimy-50 or self.y < 0\n",
    "\n",
    "        return has_hit_x and has_hit_y or exit_boundary\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.name + str((self.x,self.y))\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, xy=(400,100), game_dims=(1000,800), show=False):\n",
    "        self.jumps = 0\n",
    "        self.maxJumps = 2\n",
    "        self.xpos, self.ypos = xy\n",
    "        self.touchingObst = 0\n",
    "        self.gravityPull = 0.5\n",
    "        self.gravityCurrent = 0\n",
    "        self.xCurrent = 0\n",
    "        self.show = show\n",
    "        self.dimx, self.dimy = game_dims\n",
    "    def jump(self):\n",
    "        if self.jumps < self.maxJumps:\n",
    "            self.gravityCurrent = -10\n",
    "            self.jumps = self.jumps + 1\n",
    "    def left(self):\n",
    "        if self.touchingObst == 0:\n",
    "            self.xCurrent = -10\n",
    "    def right(self):\n",
    "        if self.touchingObst == 0:\n",
    "            self.xCurrent = 10\n",
    "    def update(self):\n",
    "        # CONTROL GRAVITY\n",
    "        self.gravityCurrent = self.gravityCurrent + self.gravityPull\n",
    "\n",
    "        # RATE OF DECREASE OF LEFT/RIGHT MOVEMENTS\n",
    "        if self.xCurrent > 0:\n",
    "            self.xCurrent = self.xCurrent - 0.5\n",
    "        if self.xCurrent < 0:\n",
    "            self.xCurrent = self.xCurrent + 0.5\n",
    "\n",
    "        # UPDATE XY COORDINATES\n",
    "        self.ypos = self.ypos + self.gravityCurrent\n",
    "        self.xpos = self.xpos + self.xCurrent\n",
    "\n",
    "        # BOUNDARIES\n",
    "        if self.xpos > self.dimx-50:\n",
    "            self.xpos = self.dimx-50\n",
    "        if self.xpos < 000:\n",
    "            self.xpos = 000\n",
    "        if self.ypos > self.dimy-50:\n",
    "            self.ypos = self.dimy-50+1\n",
    "            self.gravityCurrent = 0\n",
    "            self.jumps = 0\n",
    "            \n",
    "    def display(self, gameDisplay):\n",
    "        if self.show:\n",
    "            pygame.draw.rect(gameDisplay, red, (self.xpos, self.ypos, AGENT_SIZE, AGENT_SIZE))\n",
    "    def act(self, agent_action):\n",
    "        # print(agent_action)\n",
    "        if agent_action == 0:\n",
    "            self.left()\n",
    "        elif agent_action == 1:\n",
    "            self.right()\n",
    "        elif agent_action == 2:\n",
    "            self.jump()\n",
    "        self.update()\n",
    "            \n",
    "\n",
    "class Env:\n",
    "    def __init__(self, \n",
    "                 game_dims=(1000, 800),\n",
    "                 show=False):\n",
    "        self.dimx, self.dimy = game_dims\n",
    "        self.agent = Agent((400,100), show=show, game_dims=game_dims)\n",
    "        self.set_default_rewards()\n",
    "        self.observation_space = 5\n",
    "        self.show = show\n",
    "        pygame.init()\n",
    "        self.play = True\n",
    "        \n",
    "        # GAME DIMENSIONS\n",
    "        self.game_dims = game_dims\n",
    "        self.generator_action_space = spaces.Box(np.array([0,0,0,0]), np.array([2,self.dimx,self.dimy,360]), dtype=np.float32)\n",
    "        self.agent_action_space = spaces.Discrete(3)\n",
    "        min_obs = np.array([0]*5 + [0,0,5,0]*5)\n",
    "        max_obs = np.array([2,self.dimx,self.dimy,1,self.dimy*2] + [self.dimx,self.dimy,50,360]*5)\n",
    "        self.observation_space = spaces.Box(min_obs, max_obs, dtype=np.float32)\n",
    "        \n",
    "        # DELAY FOR WEAPON ENTITIES\n",
    "        self.delay = 0\n",
    "#         self.entity_list = []\n",
    "        \n",
    "        self.entity_limit = 5\n",
    "        self.entity_free_keys = [0,1,2,3,4]\n",
    "        self.entity_dict = {}\n",
    "    def get_free_key(self):\n",
    "        if len(self.entity_free_keys) > 0:\n",
    "            return self.entity_free_keys.pop(0)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # CONTROL MOVEMENTS\n",
    "    def execute(self):\n",
    "        agent_action = None\n",
    "        weapon_action = (0,0,0)\n",
    "        \n",
    "        events = pygame.event.get()\n",
    "        for event in events:\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_SPACE:\n",
    "                    agent_action = 2\n",
    "                if event.key == pygame.K_LEFT and self.agent.touchingObst == 0:\n",
    "                    agent_action = 0\n",
    "                if event.key == pygame.K_RIGHT and self.agent.touchingObst == 0:\n",
    "                    agent_action = 1\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.display.quit()\n",
    "                agent_action = -1\n",
    "                \n",
    "        action = (agent_action, weapon_action)\n",
    "        return action\n",
    "            \n",
    "    def test_agent(self):\n",
    "        \"\"\"FOR TESTING OF AGENT ACTIONS ONLY\"\"\"\n",
    "        run = True\n",
    "        while run:\n",
    "            sleep(0.01)\n",
    "            action = self.execute()\n",
    "            if action == -1:\n",
    "                break\n",
    "            self.step(action)\n",
    "            \n",
    "    def create_entity(self, weapon_action):\n",
    "        # print(weapon_action)\n",
    "        wep_type, wep_x, wep_y, angle = weapon_action\n",
    "        wep_xy = (wep_x, wep_y)\n",
    "#         print(wep_xy)\n",
    "        if self.delay != 0:\n",
    "            self.delay -= 1\n",
    "        wep_type = 1\n",
    "        if wep_type == 1 and self.delay == 0:\n",
    "            ent = Entity(str(wep_type), wep_xy, angle, 10)\n",
    "            ent_key = self.get_free_key()\n",
    "            if ent_key != None:\n",
    "                self.entity_dict[ent_key] = ent\n",
    "            self.delay = 20  # DELAY BEFORE THE NEXT ATTACK\n",
    "    def update_entities(self):\n",
    "        # UPDATE ENTITIES\n",
    "        collided = []\n",
    "        for key, ent in self.entity_dict.items():\n",
    "            collide = ent.update((self.agent.xpos, self.agent.ypos))\n",
    "            if not collide:\n",
    "                if self.show:\n",
    "#                     print(ent.x, ent.y)\n",
    "                    pygame.draw.rect(self.gameDisplay, blue, (ent.x, ent.y, WEAPON_SIZE, WEAPON_SIZE))\n",
    "            else:\n",
    "                self.agent.agent_reward = -100\n",
    "                self.generator_reward = 100\n",
    "                collided.append(key)\n",
    "        \n",
    "        for ent_key in collided:\n",
    "            self.entity_free_keys.append(ent_key)\n",
    "            del self.entity_dict[ent_key]\n",
    "    def display_game(self):\n",
    "        if self.show:\n",
    "#             print(self.game_dims)\n",
    "            self.gameDisplay = pygame.display.set_mode(self.game_dims, 0, 32)\n",
    "            self.gameDisplay.fill(white)\n",
    "    def display_background(self):\n",
    "        # DISPLAY BACKGROUND\n",
    "        if self.show:\n",
    "            pygame.font.init()\n",
    "            myFont = pygame.font.SysFont('Futura PT Light', 60)\n",
    "            textsurface = myFont.render('The Chosen One', False, black)\n",
    "            self.gameDisplay.blit(textsurface, (200,200))\n",
    "            pygame.display.update()\n",
    "    def set_default_rewards(self):\n",
    "        self.agent.agent_reward = 1\n",
    "        self.generator_reward = -1\n",
    "\n",
    "    def step(self, action):\n",
    "        # SET DEFAULT REWARDS FOR AGENT AND GENERATOR\n",
    "        self.set_default_rewards()\n",
    "        \n",
    "        # DISPLAY GAME\n",
    "        self.display_game()\n",
    "        \n",
    "        agent_action, weapon_action = action\n",
    "        \n",
    "        # MOVE THE AGENT\n",
    "        self.agent.act(agent_action)\n",
    "        if self.show:\n",
    "            self.agent.display(self.gameDisplay)\n",
    "        \n",
    "        # CREATE WEAPON ENTITY\n",
    "        self.create_entity(weapon_action)\n",
    "        \n",
    "        # UPDATE ENTITIES\n",
    "        self.update_entities()\n",
    "        \n",
    "        # DISPLAY BACKGROUND\n",
    "        self.display_background()\n",
    "        \n",
    "        \"\"\"RETURNS:\n",
    "        reward - (agent_reward, generator_reward)\n",
    "        state - getGameState()\n",
    "        done - CURRENT: DEFAULT: False\n",
    "        done - TODO: whether game is completed, e.g. HP <= 0\n",
    "        \"\"\"\n",
    "        reward = (self.agent.agent_reward, self.generator_reward)\n",
    "        state = self.getGameState()\n",
    "        \n",
    "        return (reward, state, False)\n",
    "        \n",
    "    def getGameState(self):\n",
    "        a = self.agent\n",
    "        agent_values = np.array([\n",
    "            a.jumps,\n",
    "            a.xpos//1000,\n",
    "            a.ypos//1000,\n",
    "            a.touchingObst,\n",
    "            a.gravityCurrent,\n",
    "            # TODO: height, width, dy, dx, direction, bounding box\n",
    "        ])\n",
    "        entity_values = np.array([])\n",
    "        for i in range(5):\n",
    "            if i in self.entity_dict:\n",
    "                e = self.entity_dict[i]\n",
    "                vals = [e.x//1000, e.y//1000, e.speed, e.angle]\n",
    "            else:\n",
    "                vals = [0,0,0,0]\n",
    "            entity_values = np.append(entity_values, vals)\n",
    "\n",
    "        values = np.append(agent_values, entity_values)\n",
    "        return values\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the game. Returns (reward, state, done).\"\"\"\n",
    "        self.__init__(game_dims=self.game_dims, show=self.show)\n",
    "        return self.getGameState()\n",
    "\n",
    "    def test_step(self):\n",
    "        # Create Gun at random place and angles\n",
    "        agent_action = random.randint(0,2)\n",
    "\n",
    "        wep_type = 1  # gun\n",
    "        wep_xy = (50, 700)  # coordinate appears at\n",
    "        angle = 0\n",
    "\n",
    "        generator_action = (wep_type, wep_xy, angle)\n",
    "        action = (agent_action, generator_action)\n",
    "        self.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(25,)\n",
      "Agent action space: Discrete(3)\n",
      "Generator action space: Box(4,)\n",
      "State: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n"
     ]
    }
   ],
   "source": [
    "env = Env(game_dims=(700, 500), show=True)\n",
    "\n",
    "# SHOW ENVIRONMENT VALUES\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Agent action space:', env.agent_action_space)\n",
    "print('Generator action space:', env.generator_action_space)\n",
    "\n",
    "state = env.reset()\n",
    "print(\"State:\", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Reward: 1\n",
      "Generator Reward: -1\n",
      "State: [ 0.          0.          0.          0.          0.5         0.\n",
      "  0.         10.         -0.34906585  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n",
      "Done: False\n"
     ]
    }
   ],
   "source": [
    "# Create Gun at random place and angles\n",
    "agent_action = random.randint(0,2)\n",
    "\n",
    "wep_type = 1  # gun\n",
    "wep_xy = (50, 700)\n",
    "wep_x, wep_y = wep_xy\n",
    "angle = 20\n",
    "\n",
    "generator_action = (wep_type, wep_x, wep_y, angle)\n",
    "action = (agent_action, generator_action)\n",
    "reward, state, done = env.step(action)\n",
    "print(\"Agent Reward:\", reward[0])\n",
    "print(\"Generator Reward:\", reward[1])\n",
    "print(\"State:\", state)\n",
    "print(\"Done:\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR CONTINUOUS ACTION SPACE\n",
    "obs_space = env.observation_space\n",
    "obs_size = obs_space.low.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"AGENT HYPERPAREMETERS\"\"\"\n",
    "agent_action_space = env.agent_action_space\n",
    "agent_action_size = agent_action_space.n\n",
    "\n",
    "# Q FUNCTION AND ADAM OPTIMIZER\n",
    "agent_q_func = chainerrl.q_functions.FCStateQFunctionWithDiscreteAction(\n",
    "    obs_size, agent_action_size,\n",
    "    n_hidden_layers=5, n_hidden_channels=100)\n",
    "\n",
    "# Use Adam to optimize q_func. eps=1e-2 is for stability.\n",
    "agent_optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "agent_optimizer.setup(agent_q_func)\n",
    "\n",
    "# Set the discount factor that discounts future rewards.\n",
    "agent_gamma = 0.95\n",
    "\n",
    "# Use epsilon-greedy for exploration\n",
    "agent_explorer = chainerrl.explorers.ConstantEpsilonGreedy(\n",
    "    epsilon=0.3, random_action_func=env.agent_action_space.sample)\n",
    "\n",
    "# DQN uses Experience Replay.\n",
    "# Specify a replay buffer and its capacity.\n",
    "agent_replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n",
    "\n",
    "# Since observations from CartPole-v0 is numpy.float64 while\n",
    "# Chainer only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi.\n",
    "agent_phi = lambda x: x.astype(np.float32, copy=False)\n",
    "\n",
    "# CHOSEN ONE AGENT\n",
    "chosen_one = chainerrl.agents.DoubleDQN(\n",
    "    agent_q_func, agent_optimizer, agent_replay_buffer, agent_gamma, agent_explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=agent_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GENERATOR HYPERPARAMETERS\"\"\"\n",
    "generator_action_space = env.generator_action_space\n",
    "generator_action_size = generator_action_space.low.size\n",
    "\n",
    "# Q FUNCTION FOR CONTINUOUS VARIABLES\n",
    "generator_q_func = chainerrl.q_functions.FCQuadraticStateQFunction(\n",
    "    obs_size, generator_action_size,\n",
    "    n_hidden_layers=5,\n",
    "    n_hidden_channels=100,\n",
    "    action_space=generator_action_space,\n",
    ")\n",
    "\n",
    "# Use the Ornstein-Uhlenbeck process for exploration\n",
    "generator_ou_sigma = (generator_action_space.high - generator_action_space.low) * 0.2\n",
    "generator_explorer = explorers.AdditiveOU(sigma=generator_ou_sigma)\n",
    "\n",
    "# Use Adam to optimize q_func. eps=1e-2 is for stability.\n",
    "generator_optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "generator_optimizer.setup(generator_q_func)\n",
    "\n",
    "# Set the discount factor that discounts future rewards.\n",
    "generator_gamma = 0.95\n",
    "\n",
    "# DQN uses Experience Replay.\n",
    "# Specify a replay buffer and its capacity.\n",
    "generator_replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n",
    "\n",
    "# Since observations from CartPole-v0 is numpy.float64 while\n",
    "# Chainer only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi.\n",
    "generator_phi = lambda x: x.astype(np.float32, copy=False)\n",
    "\n",
    "# GENERATOR AGENT\n",
    "generator = chainerrl.agents.DoubleDQN(\n",
    "    generator_q_func, generator_optimizer, generator_replay_buffer, generator_gamma, generator_explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=generator_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_action = chosen_one.act_and_train(obs, agent_reward)\n",
    "generator_action = generator.act_and_train(obs, generator_reward)\n",
    "action = (agent_action, generator_action)\n",
    "\n",
    "(reward, obs, done) = env.step(action)\n",
    "agent_reward, generator_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-428-608b9f596e02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#         print(obs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0magent_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchosen_one\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_and_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mgenerator_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_and_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0magent_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\the-chosen-one\\lib\\site-packages\\chainerrl\\agents\\dqn.py\u001b[0m in \u001b[0;36mact_and_train\u001b[1;34m(self, obs, reward)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_updater\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_if_necessary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't:%s r:%s a:%s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\the-chosen-one\\lib\\site-packages\\chainerrl\\replay_buffer.py\u001b[0m in \u001b[0;36mupdate_if_necessary\u001b[1;34m(self, iteration)\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m                 \u001b[0mtransitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\the-chosen-one\\lib\\site-packages\\chainerrl\\agents\\dqn.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, experiences, errors_out)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleargrads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\the-chosen-one\\lib\\site-packages\\chainer\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, retain_grad, enable_double_backprop, loss_scale)\u001b[0m\n\u001b[0;32m   1436\u001b[0m             \u001b[1;31m# to _backprop_to_all, but it is working because grad_var is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1437\u001b[0m             \u001b[1;31m# immediately popped away as None = _backprop_utils._reduce([None])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1438\u001b[1;33m             \u001b[0m_backprop_to_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\the-chosen-one\\lib\\site-packages\\chainer\\variable.py\u001b[0m in \u001b[0;36m_backprop_to_all\u001b[1;34m(outputs, retain_grad, loss_scale)\u001b[0m\n\u001b[0;32m   1652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1653\u001b[0m             _backprop_utils.backprop_step(\n\u001b[1;32m-> 1654\u001b[1;33m                 func, target_input_indexes, out_grad, in_grad, is_debug)\n\u001b[0m\u001b[0;32m   1655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\the-chosen-one\\lib\\site-packages\\chainer\\_backprop_utils.py\u001b[0m in \u001b[0;36mbackprop_step\u001b[1;34m(func, target_input_indexes, grad_outputs, grad_inputs, is_debug)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             gxs = func.backward(\n\u001b[1;32m--> 138\u001b[1;33m                 target_input_indexes, grad_outputs)\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0m_reraise_with_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\the-chosen-one\\lib\\site-packages\\chainer\\functions\\math\\matmul.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, indexes, grad_outputs)\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 ga, = MatMul(self.transc, not self.transb, self.transa,\n\u001b[1;32m--> 160\u001b[1;33m                              a.dtype).apply((gy, b))\n\u001b[0m\u001b[0;32m    161\u001b[0m                 \u001b[0mga\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mga\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mga\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\the-chosen-one\\lib\\site-packages\\chainer\\function_node.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m                 \u001b[1;31m# In normal case, simply run the forward method.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# Check for output array types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\the-chosen-one\\lib\\site-packages\\chainer\\functions\\math\\matmul.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;31m# may broadcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\the-chosen-one\\lib\\site-packages\\chainer\\functions\\math\\matmul.py\u001b[0m in \u001b[0;36m_matmul\u001b[1;34m(a, b, transa, transb, transout)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'matmul'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# numpy.matmul is supported from version 1.10.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MAIN TRAINING LOOP\n",
    "n_episodes = 100\n",
    "max_episode_len = 1000\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    agent_reward = 0\n",
    "    generator_reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while not done and t < max_episode_len:\n",
    "        # Uncomment to watch the behaviour\n",
    "        # env.render()\n",
    "#         print(obs)\n",
    "        agent_action = chosen_one.act_and_train(obs, agent_reward)\n",
    "        generator_action = generator.act_and_train(obs, generator_reward)\n",
    "        action = (agent_action, generator_action)\n",
    "        \n",
    "        (reward, obs, done) = env.step(action)\n",
    "        agent_reward, generator_reward = reward\n",
    "\n",
    "        R += agent_reward\n",
    "        t += 1\n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i,\n",
    "              'R:', R,\n",
    "              'agent statistics:', chosen_one.get_statistics()\n",
    "              'generator statistics:', generator.get_statistics())\n",
    "    chosen_one.stop_episode_and_train(obs, agent_reward, done)\n",
    "    generator.stop_episode_and_train(obs, generator_reward, done)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-chosen-one",
   "language": "python",
   "name": "the-chosen-one"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
