{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainerrl\n",
    "from chainerrl import explorers\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "from time import sleep\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "# COLORS\n",
    "white = (255, 255, 255)\n",
    "black = (0, 0, 0)\n",
    "red = (255, 0, 0)\n",
    "green = (0, 255, 0)\n",
    "blue = (0, 0, 255)\n",
    "purple = (255, 0, 255)\n",
    "\n",
    "# SIZES\n",
    "AGENT_SIZE = 50\n",
    "WEAPON_SIZE = 20\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, name, xy, angle, speed, game_dims=(1000,800)):\n",
    "        self.name = name\n",
    "        self.x, self.y = xy\n",
    "        self.speed = speed\n",
    "        self.angle = math.radians(-angle)  # -1 to 1 \n",
    "        self.dx = self.speed * math.cos(self.angle)\n",
    "        self.dy = self.speed * math.sin(self.angle)\n",
    "        self.dimx, self.dimy = game_dims\n",
    "        \n",
    "    def update(self, agent_xy):\n",
    "        self.x += self.dx\n",
    "        self.y += self.dy\n",
    "\n",
    "        agent_x, agent_y = agent_xy\n",
    "\n",
    "        has_hit_x = self.x >= agent_x - WEAPON_SIZE and self.x <= agent_x + AGENT_SIZE\n",
    "        has_hit_y = self.y >= agent_y - WEAPON_SIZE and self.y <= agent_y + AGENT_SIZE\n",
    "        \n",
    "        exit_boundary = self.x > self.dimx-50 or self.x < 000 or self.y > self.dimy-50 or self.y < 0\n",
    "\n",
    "        return has_hit_x and has_hit_y or exit_boundary\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.name + str((self.x,self.y))\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, xy=(400,100), game_dims=(1000,800), show=False):\n",
    "        self.jumps = 0\n",
    "        self.maxJumps = 2\n",
    "        self.xpos, self.ypos = xy\n",
    "        self.touchingObst = 0\n",
    "        self.gravityPull = 0.5\n",
    "        self.gravityCurrent = 0\n",
    "        self.xCurrent = 0\n",
    "        self.show = show\n",
    "        self.dimx, self.dimy = game_dims\n",
    "    def jump(self):\n",
    "        if self.jumps < self.maxJumps:\n",
    "            self.gravityCurrent = -10\n",
    "            self.jumps = self.jumps + 1\n",
    "    def left(self):\n",
    "        if self.touchingObst == 0:\n",
    "            self.xCurrent = -10\n",
    "    def right(self):\n",
    "        if self.touchingObst == 0:\n",
    "            self.xCurrent = 10\n",
    "    def update(self):\n",
    "        # CONTROL GRAVITY\n",
    "        self.gravityCurrent = self.gravityCurrent + self.gravityPull\n",
    "\n",
    "        # RATE OF DECREASE OF LEFT/RIGHT MOVEMENTS\n",
    "        if self.xCurrent > 0:\n",
    "            self.xCurrent = self.xCurrent - 0.5\n",
    "        if self.xCurrent < 0:\n",
    "            self.xCurrent = self.xCurrent + 0.5\n",
    "\n",
    "        # UPDATE XY COORDINATES\n",
    "        self.ypos = self.ypos + self.gravityCurrent\n",
    "        self.xpos = self.xpos + self.xCurrent\n",
    "\n",
    "        # BOUNDARIES\n",
    "        if self.xpos > self.dimx-50:\n",
    "            self.xpos = self.dimx-50\n",
    "        if self.xpos < 000:\n",
    "            self.xpos = 000\n",
    "        if self.ypos > self.dimy-50:\n",
    "            self.ypos = self.dimy-50+1\n",
    "            self.gravityCurrent = 0\n",
    "            self.jumps = 0\n",
    "            \n",
    "    def display(self, gameDisplay):\n",
    "        if self.show:\n",
    "            pygame.draw.rect(gameDisplay, red, (self.xpos, self.ypos, AGENT_SIZE, AGENT_SIZE))\n",
    "    def act(self, agent_action):\n",
    "        # print(agent_action)\n",
    "        if agent_action == 0:\n",
    "            self.left()\n",
    "        elif agent_action == 1:\n",
    "            self.right()\n",
    "        elif agent_action == 2:\n",
    "            self.jump()\n",
    "        self.update()\n",
    "            \n",
    "\n",
    "class Env:\n",
    "    def __init__(self, \n",
    "                 game_dims=(1000, 800),\n",
    "                 show=False):\n",
    "        self.dimx, self.dimy = game_dims\n",
    "        self.agent = Agent((400,100), show=show, game_dims=game_dims)\n",
    "        self.set_default_rewards()\n",
    "        self.observation_space = 5\n",
    "        self.show = show\n",
    "        pygame.init()\n",
    "        self.play = True\n",
    "        \n",
    "        # GAME DIMENSIONS\n",
    "        self.game_dims = game_dims\n",
    "        self.generator_action_space = spaces.Box(np.array([0,0,0,0]), np.array([2,self.dimx,self.dimy,360]), dtype=np.float32)\n",
    "        self.agent_action_space = spaces.Discrete(3)\n",
    "        min_obs = np.array([0]*5 + [0,0,5,0]*5)\n",
    "        max_obs = np.array([2,self.dimx,self.dimy,1,self.dimy*2] + [self.dimx,self.dimy,50,360]*5)\n",
    "        self.observation_space = spaces.Box(min_obs, max_obs, dtype=np.float32)\n",
    "        \n",
    "        # DELAY FOR WEAPON ENTITIES\n",
    "        self.delay = 0\n",
    "#         self.entity_list = []\n",
    "        \n",
    "        self.entity_limit = 5\n",
    "        self.entity_free_keys = [0,1,2,3,4]\n",
    "        self.entity_dict = {}\n",
    "    def get_free_key(self):\n",
    "        if len(self.entity_free_keys) > 0:\n",
    "            return self.entity_free_keys.pop(0)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # CONTROL MOVEMENTS\n",
    "    def execute(self):\n",
    "        agent_action = None\n",
    "        weapon_action = (0,0,0)\n",
    "        \n",
    "        events = pygame.event.get()\n",
    "        for event in events:\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_SPACE:\n",
    "                    agent_action = 2\n",
    "                if event.key == pygame.K_LEFT and self.agent.touchingObst == 0:\n",
    "                    agent_action = 0\n",
    "                if event.key == pygame.K_RIGHT and self.agent.touchingObst == 0:\n",
    "                    agent_action = 1\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.display.quit()\n",
    "                agent_action = -1\n",
    "                \n",
    "        action = (agent_action, weapon_action)\n",
    "        return action\n",
    "            \n",
    "    def test_agent(self):\n",
    "        \"\"\"FOR TESTING OF AGENT ACTIONS ONLY\"\"\"\n",
    "        run = True\n",
    "        while run:\n",
    "            sleep(0.01)\n",
    "            action = self.execute()\n",
    "            if action == -1:\n",
    "                break\n",
    "            self.step(action)\n",
    "            \n",
    "    def create_entity(self, weapon_action):\n",
    "        # print(weapon_action)\n",
    "        wep_type, wep_x, wep_y, angle = weapon_action\n",
    "        wep_xy = (wep_x, wep_y)\n",
    "#         print(wep_xy)\n",
    "        if self.delay != 0:\n",
    "            self.delay -= 1\n",
    "        wep_type = 1\n",
    "        if wep_type == 1 and self.delay == 0:\n",
    "            ent = Entity(str(wep_type), wep_xy, angle, 10)\n",
    "            ent_key = self.get_free_key()\n",
    "            if ent_key != None:\n",
    "                self.entity_dict[ent_key] = ent\n",
    "            self.delay = 20  # DELAY BEFORE THE NEXT ATTACK\n",
    "    def update_entities(self):\n",
    "        # UPDATE ENTITIES\n",
    "        collided = []\n",
    "        for key, ent in self.entity_dict.items():\n",
    "            collide = ent.update((self.agent.xpos, self.agent.ypos))\n",
    "            if not collide:\n",
    "                if self.show:\n",
    "#                     print(ent.x, ent.y)\n",
    "                    pygame.draw.rect(self.gameDisplay, blue, (ent.x, ent.y, WEAPON_SIZE, WEAPON_SIZE))\n",
    "            else:\n",
    "                self.agent.agent_reward = -20\n",
    "                self.generator_reward = 20\n",
    "                collided.append(key)\n",
    "        \n",
    "        for ent_key in collided:\n",
    "            self.entity_free_keys.append(ent_key)\n",
    "            del self.entity_dict[ent_key]\n",
    "    def display_game(self):\n",
    "        if self.show:\n",
    "#             print(self.game_dims)\n",
    "            self.gameDisplay = pygame.display.set_mode(self.game_dims, 0, 32)\n",
    "            self.gameDisplay.fill(white)\n",
    "    def display_background(self):\n",
    "        # DISPLAY BACKGROUND\n",
    "        if self.show:\n",
    "            pygame.font.init()\n",
    "            myFont = pygame.font.SysFont('Futura PT Light', 60)\n",
    "            textsurface = myFont.render('The Chosen One', False, black)\n",
    "            self.gameDisplay.blit(textsurface, (200,200))\n",
    "            pygame.display.update()\n",
    "    def set_default_rewards(self):\n",
    "        self.agent.agent_reward = 1\n",
    "        self.generator_reward = -1\n",
    "\n",
    "    def step(self, action):\n",
    "        # SET DEFAULT REWARDS FOR AGENT AND GENERATOR\n",
    "        self.set_default_rewards()\n",
    "        \n",
    "        # DISPLAY GAME\n",
    "        self.display_game()\n",
    "        \n",
    "        agent_action, weapon_action = action\n",
    "        \n",
    "        # MOVE THE AGENT\n",
    "        self.agent.act(agent_action)\n",
    "        if self.show:\n",
    "            self.agent.display(self.gameDisplay)\n",
    "        \n",
    "        # CREATE WEAPON ENTITY\n",
    "        self.create_entity(weapon_action)\n",
    "        \n",
    "        # UPDATE ENTITIES\n",
    "        self.update_entities()\n",
    "        \n",
    "        # DISPLAY BACKGROUND\n",
    "        self.display_background()\n",
    "        \n",
    "        \"\"\"RETURNS:\n",
    "        reward - (agent_reward, generator_reward)\n",
    "        state - getGameState()\n",
    "        done - CURRENT: DEFAULT: False\n",
    "        done - TODO: whether game is completed, e.g. HP <= 0\n",
    "        \"\"\"\n",
    "        reward = (self.agent.agent_reward, self.generator_reward)\n",
    "        state = self.getGameState()\n",
    "        \n",
    "        return (reward, state, False)\n",
    "        \n",
    "    def getGameState(self):\n",
    "        a = self.agent\n",
    "        agent_values = np.array([\n",
    "            a.jumps,\n",
    "            a.xpos//1000,\n",
    "            a.ypos//1000,\n",
    "            a.touchingObst,\n",
    "            a.gravityCurrent,\n",
    "            # TODO: height, width, dy, dx, direction, bounding box\n",
    "        ])\n",
    "        entity_values = np.array([])\n",
    "        for i in range(5):\n",
    "            if i in self.entity_dict:\n",
    "                e = self.entity_dict[i]\n",
    "                vals = [e.x//1000, e.y//1000, e.speed, e.angle]\n",
    "            else:\n",
    "                vals = [0,0,0,0]\n",
    "            entity_values = np.append(entity_values, vals)\n",
    "\n",
    "        values = np.append(agent_values, entity_values)\n",
    "        return values\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the game. Returns (reward, state, done).\"\"\"\n",
    "        self.__init__(game_dims=self.game_dims, show=self.show)\n",
    "        return self.getGameState()\n",
    "\n",
    "    def test_step(self):\n",
    "        # Create Gun at random place and angles\n",
    "        agent_action = random.randint(0,2)\n",
    "\n",
    "        wep_type = 1  # gun\n",
    "        wep_xy = (50, 700)  # coordinate appears at\n",
    "        angle = 0\n",
    "\n",
    "        generator_action = (wep_type, wep_xy, angle)\n",
    "        action = (agent_action, generator_action)\n",
    "        self.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(25,)\n",
      "Agent action space: Discrete(3)\n",
      "Generator action space: Box(4,)\n",
      "State: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n"
     ]
    }
   ],
   "source": [
    "env = Env(game_dims=(700, 500), show=False)\n",
    "\n",
    "# SHOW ENVIRONMENT VALUES\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Agent action space:', env.agent_action_space)\n",
    "print('Generator action space:', env.generator_action_space)\n",
    "\n",
    "state = env.reset()\n",
    "print(\"State:\", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Reward: 1\n",
      "Generator Reward: -1\n",
      "State: [ 0.          0.          0.          0.          0.5         0.\n",
      "  0.         10.         -0.34906585  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n",
      "Done: False\n"
     ]
    }
   ],
   "source": [
    "# Create Gun at random place and angles\n",
    "agent_action = random.randint(0,2)\n",
    "\n",
    "wep_type = 1  # gun\n",
    "wep_xy = (50, 700)\n",
    "wep_x, wep_y = wep_xy\n",
    "angle = 20\n",
    "\n",
    "generator_action = (wep_type, wep_x, wep_y, angle)\n",
    "action = (agent_action, generator_action)\n",
    "reward, state, done = env.step(action)\n",
    "print(\"Agent Reward:\", reward[0])\n",
    "print(\"Generator Reward:\", reward[1])\n",
    "print(\"State:\", state)\n",
    "print(\"Done:\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR CONTINUOUS ACTION SPACE\n",
    "obs_space = env.observation_space\n",
    "obs_size = obs_space.low.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"AGENT HYPERPAREMETERS\"\"\"\n",
    "agent_action_space = env.agent_action_space\n",
    "agent_action_size = agent_action_space.n\n",
    "\n",
    "# Q FUNCTION AND ADAM OPTIMIZER\n",
    "agent_q_func = chainerrl.q_functions.FCStateQFunctionWithDiscreteAction(\n",
    "    obs_size, agent_action_size,\n",
    "    n_hidden_layers=5, n_hidden_channels=100)\n",
    "\n",
    "# Use Adam to optimize q_func. eps=1e-2 is for stability.\n",
    "agent_optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "agent_optimizer.setup(agent_q_func)\n",
    "\n",
    "# Set the discount factor that discounts future rewards.\n",
    "agent_gamma = 0.95\n",
    "\n",
    "# Use epsilon-greedy for exploration\n",
    "agent_explorer = chainerrl.explorers.ConstantEpsilonGreedy(\n",
    "    epsilon=0.3, random_action_func=env.agent_action_space.sample)\n",
    "\n",
    "# DQN uses Experience Replay.\n",
    "# Specify a replay buffer and its capacity.\n",
    "agent_replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n",
    "\n",
    "# Since observations from CartPole-v0 is numpy.float64 while\n",
    "# Chainer only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi.\n",
    "agent_phi = lambda x: x.astype(np.float32, copy=False)\n",
    "\n",
    "# CHOSEN ONE AGENT\n",
    "chosen_one = chainerrl.agents.DoubleDQN(\n",
    "    agent_q_func, agent_optimizer, agent_replay_buffer, agent_gamma, agent_explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=agent_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GENERATOR HYPERPARAMETERS\"\"\"\n",
    "generator_action_space = env.generator_action_space\n",
    "generator_action_size = generator_action_space.low.size\n",
    "\n",
    "# Q FUNCTION FOR CONTINUOUS VARIABLES\n",
    "generator_q_func = chainerrl.q_functions.FCQuadraticStateQFunction(\n",
    "    obs_size, generator_action_size,\n",
    "    n_hidden_layers=5,\n",
    "    n_hidden_channels=100,\n",
    "    action_space=generator_action_space,\n",
    ")\n",
    "\n",
    "# Use the Ornstein-Uhlenbeck process for exploration\n",
    "generator_ou_sigma = (generator_action_space.high - generator_action_space.low) * 0.2\n",
    "generator_explorer = explorers.AdditiveOU(sigma=generator_ou_sigma)\n",
    "\n",
    "# Use Adam to optimize q_func. eps=1e-2 is for stability.\n",
    "generator_optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "generator_optimizer.setup(generator_q_func)\n",
    "\n",
    "# Set the discount factor that discounts future rewards.\n",
    "generator_gamma = 0.95\n",
    "\n",
    "# DQN uses Experience Replay.\n",
    "# Specify a replay buffer and its capacity.\n",
    "generator_replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n",
    "\n",
    "# Since observations from CartPole-v0 is numpy.float64 while\n",
    "# Chainer only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi.\n",
    "generator_phi = lambda x: x.astype(np.float32, copy=False)\n",
    "\n",
    "# GENERATOR AGENT\n",
    "generator = chainerrl.agents.DoubleDQN(\n",
    "    generator_q_func, generator_optimizer, generator_replay_buffer, generator_gamma, generator_explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=generator_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_action = chosen_one.act_and_train(obs, agent_reward)\n",
    "generator_action = generator.act_and_train(obs, generator_reward)\n",
    "action = (agent_action, generator_action)\n",
    "\n",
    "(reward, obs, done) = env.step(action)\n",
    "agent_reward, generator_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 1 R_agent: 17 agent stat: [('average_q', 8.933350225869424), ('average_loss', 1.0534251732671915), ('n_updates', 2219)]\n",
      "ep: 2 R_agent: -4 agent stat: [('average_q', 10.506435539273403), ('average_loss', 0.932203403650184), ('n_updates', 2719)]\n",
      "ep: 3 R_agent: 17 agent stat: [('average_q', 12.344744379204764), ('average_loss', 1.0980929290231225), ('n_updates', 3219)]\n",
      "ep: 4 R_agent: 17 agent stat: [('average_q', 13.576695332067278), ('average_loss', 1.013638625438399), ('n_updates', 3719)]\n",
      "ep: 5 R_agent: -4 agent stat: [('average_q', 14.52009680853569), ('average_loss', 0.9688254540884538), ('n_updates', 4219)]\n",
      "ep: 6 R_agent: -25 agent stat: [('average_q', 15.15822366882791), ('average_loss', 1.0232812214119211), ('n_updates', 4719)]\n",
      "ep: 7 R_agent: -4 agent stat: [('average_q', 15.958815863795643), ('average_loss', 1.0872832611125127), ('n_updates', 5219)]\n",
      "ep: 8 R_agent: 38 agent stat: [('average_q', 16.49151832260835), ('average_loss', 0.8900419801770801), ('n_updates', 5719)]\n",
      "ep: 9 R_agent: 59 agent stat: [('average_q', 17.07016071233688), ('average_loss', 0.9828312294077154), ('n_updates', 6219)]\n",
      "ep: 10 R_agent: -4 agent stat: [('average_q', 17.649415115218556), ('average_loss', 0.9926210895359582), ('n_updates', 6719)]\n",
      "ep: 11 R_agent: 17 agent stat: [('average_q', 17.927345786602764), ('average_loss', 0.9304875898746651), ('n_updates', 7219)]\n",
      "ep: 12 R_agent: -25 agent stat: [('average_q', 18.36267862048356), ('average_loss', 0.9893695892396479), ('n_updates', 7719)]\n",
      "ep: 13 R_agent: -4 agent stat: [('average_q', 18.38171323653501), ('average_loss', 0.9241033812323844), ('n_updates', 8219)]\n",
      "ep: 14 R_agent: -4 agent stat: [('average_q', 18.358752515614423), ('average_loss', 0.9257514607753502), ('n_updates', 8719)]\n",
      "ep: 15 R_agent: -4 agent stat: [('average_q', 18.324054150678013), ('average_loss', 0.9796099206748538), ('n_updates', 9219)]\n",
      "ep: 16 R_agent: 17 agent stat: [('average_q', 18.468055120682102), ('average_loss', 0.8764380768338963), ('n_updates', 9719)]\n",
      "ep: 17 R_agent: 17 agent stat: [('average_q', 18.40876960014388), ('average_loss', 0.9224897733005196), ('n_updates', 10219)]\n",
      "ep: 18 R_agent: -25 agent stat: [('average_q', 18.351989297913285), ('average_loss', 0.9407309778728097), ('n_updates', 10719)]\n",
      "ep: 19 R_agent: -4 agent stat: [('average_q', 18.327562116262236), ('average_loss', 1.0339956383904254), ('n_updates', 11219)]\n",
      "ep: 20 R_agent: -4 agent stat: [('average_q', 18.22262610074285), ('average_loss', 1.0883011594102543), ('n_updates', 11719)]\n",
      "ep: 21 R_agent: 38 agent stat: [('average_q', 18.31999661847396), ('average_loss', 0.9373649253868024), ('n_updates', 12219)]\n",
      "ep: 22 R_agent: 17 agent stat: [('average_q', 18.162410679965834), ('average_loss', 0.92698005620694), ('n_updates', 12719)]\n",
      "ep: 23 R_agent: 17 agent stat: [('average_q', 18.08571917505515), ('average_loss', 0.9713540538745288), ('n_updates', 13219)]\n",
      "ep: 24 R_agent: 17 agent stat: [('average_q', 18.27764327278433), ('average_loss', 0.9662288727795417), ('n_updates', 13719)]\n",
      "ep: 25 R_agent: 17 agent stat: [('average_q', 18.52130497142189), ('average_loss', 1.0300507824580623), ('n_updates', 14219)]\n",
      "ep: 26 R_agent: -4 agent stat: [('average_q', 18.854523464316387), ('average_loss', 0.9430185265062379), ('n_updates', 14719)]\n",
      "ep: 27 R_agent: 17 agent stat: [('average_q', 18.889517459274906), ('average_loss', 1.0415091411861097), ('n_updates', 15219)]\n",
      "ep: 28 R_agent: -4 agent stat: [('average_q', 18.715206332707396), ('average_loss', 1.0126274863866327), ('n_updates', 15719)]\n",
      "ep: 29 R_agent: 17 agent stat: [('average_q', 18.790063509365993), ('average_loss', 1.0532536430340882), ('n_updates', 16219)]\n",
      "ep: 30 R_agent: -4 agent stat: [('average_q', 18.70721426897902), ('average_loss', 0.9260108351577505), ('n_updates', 16719)]\n",
      "ep: 31 R_agent: 38 agent stat: [('average_q', 18.561791233851455), ('average_loss', 1.010899353263316), ('n_updates', 17219)]\n",
      "ep: 32 R_agent: -25 agent stat: [('average_q', 18.482128248597974), ('average_loss', 1.068152129594581), ('n_updates', 17719)]\n",
      "ep: 33 R_agent: -4 agent stat: [('average_q', 18.49925779405052), ('average_loss', 1.020417414245881), ('n_updates', 18219)]\n",
      "ep: 34 R_agent: 38 agent stat: [('average_q', 18.459407287397653), ('average_loss', 1.0044050840239398), ('n_updates', 18719)]\n",
      "ep: 35 R_agent: 38 agent stat: [('average_q', 18.349208766955556), ('average_loss', 1.0117042778923429), ('n_updates', 19219)]\n",
      "ep: 36 R_agent: -4 agent stat: [('average_q', 18.12014691700434), ('average_loss', 1.0290722124953957), ('n_updates', 19719)]\n",
      "ep: 37 R_agent: 38 agent stat: [('average_q', 17.948434243427222), ('average_loss', 0.9839583608853858), ('n_updates', 20219)]\n",
      "ep: 38 R_agent: -4 agent stat: [('average_q', 17.8867843082053), ('average_loss', 0.9357174314837832), ('n_updates', 20719)]\n",
      "ep: 39 R_agent: -4 agent stat: [('average_q', 17.79101802245386), ('average_loss', 0.9959536569504638), ('n_updates', 21219)]\n",
      "ep: 40 R_agent: -25 agent stat: [('average_q', 17.95060479705685), ('average_loss', 0.9996578637711315), ('n_updates', 21719)]\n",
      "ep: 41 R_agent: -25 agent stat: [('average_q', 17.900130786065674), ('average_loss', 0.941011484020759), ('n_updates', 22219)]\n",
      "ep: 42 R_agent: -4 agent stat: [('average_q', 18.0676108367893), ('average_loss', 0.9260305483812491), ('n_updates', 22719)]\n",
      "ep: 43 R_agent: -25 agent stat: [('average_q', 18.308893858799145), ('average_loss', 0.9410893808659516), ('n_updates', 23219)]\n",
      "ep: 44 R_agent: 17 agent stat: [('average_q', 18.375931750465114), ('average_loss', 0.9652657636426308), ('n_updates', 23719)]\n",
      "ep: 45 R_agent: 17 agent stat: [('average_q', 18.52305422105082), ('average_loss', 0.9027208460948997), ('n_updates', 24219)]\n",
      "ep: 46 R_agent: 17 agent stat: [('average_q', 18.530239966310965), ('average_loss', 0.9768954163105528), ('n_updates', 24719)]\n",
      "ep: 47 R_agent: 59 agent stat: [('average_q', 18.57470803575141), ('average_loss', 0.9402612490170089), ('n_updates', 25219)]\n",
      "ep: 48 R_agent: -4 agent stat: [('average_q', 18.674687977138603), ('average_loss', 0.9913902318546998), ('n_updates', 25719)]\n",
      "ep: 49 R_agent: 17 agent stat: [('average_q', 18.509619524807256), ('average_loss', 1.0413980995701568), ('n_updates', 26219)]\n",
      "ep: 50 R_agent: 38 agent stat: [('average_q', 18.39841410607947), ('average_loss', 0.9702957640149605), ('n_updates', 26719)]\n",
      "ep: 51 R_agent: -4 agent stat: [('average_q', 18.30110770239312), ('average_loss', 0.9510091494511359), ('n_updates', 27219)]\n",
      "ep: 52 R_agent: 17 agent stat: [('average_q', 18.398024880958207), ('average_loss', 0.9256605684227486), ('n_updates', 27719)]\n",
      "ep: 53 R_agent: 17 agent stat: [('average_q', 18.449127837953405), ('average_loss', 1.0074603276611067), ('n_updates', 28219)]\n",
      "ep: 54 R_agent: 17 agent stat: [('average_q', 18.51347426810471), ('average_loss', 0.9581800967060667), ('n_updates', 28719)]\n",
      "ep: 55 R_agent: 17 agent stat: [('average_q', 18.55975539832565), ('average_loss', 0.9787316866194602), ('n_updates', 29219)]\n",
      "ep: 56 R_agent: 17 agent stat: [('average_q', 18.485683085222558), ('average_loss', 0.9476107421937898), ('n_updates', 29719)]\n"
     ]
    }
   ],
   "source": [
    "# MAIN TRAINING LOOP\n",
    "n_episodes = 1000\n",
    "max_episode_len = 500\n",
    "\n",
    "R_agent_history = []\n",
    "R_generator_history = []\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    agent_reward = 0\n",
    "    generator_reward = 0\n",
    "    done = False\n",
    "    R_agent = 0  # return (sum of rewards)\n",
    "    R_generator = 0\n",
    "    t = 0  # time step\n",
    "    while not done and t < max_episode_len:\n",
    "        # Uncomment to watch the behaviour\n",
    "        # env.render()\n",
    "#         print(obs)\n",
    "        agent_action = chosen_one.act_and_train(obs, agent_reward)\n",
    "        generator_action = generator.act_and_train(obs, generator_reward)\n",
    "        action = (agent_action, generator_action)\n",
    "        \n",
    "        (reward, obs, done) = env.step(action)\n",
    "        agent_reward, generator_reward = reward\n",
    "\n",
    "        R_agent += agent_reward\n",
    "        R_generator += generator_reward\n",
    "        R_agent_history.append(R_agent)\n",
    "        R_generator_history.append(R_generator)\n",
    "        t += 1\n",
    "    if i % 1 == 0:\n",
    "        print('ep:', i,\n",
    "              'R_agent:', R_agent,\n",
    "#               'R_generator:', R_generator,\n",
    "              'agent stat:', chosen_one.get_statistics())\n",
    "#               'generator statistics:', generator.get_statistics())\n",
    "    chosen_one.stop_episode_and_train(obs, agent_reward, done)\n",
    "    generator.stop_episode_and_train(obs, generator_reward, done)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-chosen-one",
   "language": "python",
   "name": "the-chosen-one"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
